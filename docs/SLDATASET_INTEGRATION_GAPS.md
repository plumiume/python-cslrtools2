# SLDatasetçµ±åˆæ™‚ã®ä¸è¶³å®šç¾©åˆ†æ

**ä½œæˆæ—¥**: 2025å¹´12æœˆ1æ—¥  
**ç›®çš„**: metrics_prototypeã‚’SLDatasetã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆã™ã‚‹éš›ã«å¿…è¦ã¨ãªã‚‹å®šç¾©ã‚’ç‰¹å®š

---

## ğŸ“‹ ç¾çŠ¶åˆ†æ

### SLDatasetã®èª­ã¿è¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ æ§‹é€ 

```mermaid
graph TB
    subgraph "SLDataset Core"
        Dataset[SLDataset<br/>from_zarr]
        Item[SLDatasetItem<br/>from_zarr]
        Holder[SLKeyHolder<br/>Type Guards]
    end
    
    subgraph "Array Loaders"
        PreKey[PreKeyLoader<br/>å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«]
        Container[ContainerLoader<br/>è¤‡æ•°é…åˆ—]
    end
    
    subgraph "Utilities"
        Utils[get_group<br/>get_array<br/>as_tensor]
    end
    
    subgraph "Concrete Loaders"
        Npy[NpyLoader]
        Npz[NpzLoader]
        Torch[TorchLoader]
        Zarr[ZarrLoader]
    end
    
    Dataset -->|uses| Item
    Dataset -->|uses| Utils
    Item -->|uses| PreKey
    Item -->|uses| Container
    PreKey -.implements.-> Npy
    Container -.implements.-> Npz
    Container -.implements.-> Torch
    Container -.implements.-> Zarr
    
    style Dataset fill:#e1f5ff
    style Item fill:#fff4e1
    style Utils fill:#e8f5e9
    style PreKey fill:#f3e5f5
    style Container fill:#f3e5f5
```

---

## ğŸ” metrics_prototypeã®ç¾åœ¨ã®è¦ä»¶

### å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
# metrics_prototypeãŒå¿…è¦ã¨ã™ã‚‹æ©Ÿèƒ½
landmarks_dict = {
    "mediapipe.pose": np.ndarray,        # shape: (T, 33, 4)
    "mediapipe.left_hand": np.ndarray,   # shape: (T, 21, 3)
    "mediapipe.right_hand": np.ndarray,  # shape: (T, 21, 3)
}

# ç¾åœ¨ã®demo.pyã§ã®å®Ÿè£…
for i in range(num_samples):
    landmarks = load_all_landmarks_from_zarr(zarr_path, i)
    metric.calculate(landmarks["mediapipe.pose"])
```

### SLDatasetã§å¯èƒ½ãªã‚¢ã‚¯ã‚»ã‚¹

```python
# SLDatasetã®æ¨™æº–çš„ãªä½¿ã„æ–¹
dataset = SLDataset.from_zarr(root)
item = dataset[0]  # SLDatasetItem

# item.landmarks ã®æ§‹é€ 
item.landmarks: Mapping[Klm, zarr.Array]
# ä¾‹: {"mediapipe.pose": zarr.Array, "mediapipe.left_hand": zarr.Array}
```

---

## âŒ ä¸»ãªä¸è¶³ç‚¹ã¨èª¤è§£ã®è¨‚æ­£

### 1. âœ… **è¨‚æ­£**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŠ¹ç‡ã«ã¤ã„ã¦

**ç¾çŠ¶ã®SLDataset.from_zarr()ã®å®Ÿè£…**:

```python
# src/cslrtools2/sldataset/dataset/core.py line 186
items: Sequence[ZarrSLDatasetItem[Kvid, Klm, Ktgt]] = []
for item_group in get_group(group, "items").group_values():
    items.append(
        SLDatasetItem[...].from_zarr(item_group)
    )
```

**èª¤è§£ã—ã¦ã„ãŸç‚¹**:
- âŒ **èª¤è§£**: å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€ã¨è€ƒãˆã¦ã„ãŸ
- âœ… **å®Ÿéš›**: `SLDatasetItem.from_zarr()` ã¯ zarr.Array ã¸ã®**å‚ç…§**ã®ã¿ã‚’è¿”ã™
- âœ… **å®Ÿéš›**: zarr.Array ã¯é…å»¶è©•ä¾¡ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã¯ `np.array(zarr_array)` æ™‚ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ï¼‰

**SLDatasetItem.from_zarr() ã®å®Ÿè£…ç¢ºèª**:

```python
# src/cslrtools2/sldataset/dataset/item.py line 255-275
@classmethod
def from_zarr(cls, group: zarr.Group) -> ZarrSLDatasetItem[Kvid, Klm, Ktgt]:
    videos = cls._load_category_from_zarr(...)
    landmarks = cls._load_category_from_zarr(...)
    targets = cls._load_category_from_zarr(...)
    
    return SLDatasetItem(videos=videos, landmarks=landmarks, targets=targets)

# _load_category_from_zarr() ã¯ get_array() ã‚’ä½¿ç”¨
# â†’ zarr.Array ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ï¼ˆãƒ‡ãƒ¼ã‚¿æœ¬ä½“ã¯èª­ã¿è¾¼ã¾ãªã„ï¼‰
```

**çµè«–**:
- âœ… **æ—¢å­˜ã®SLDatasetã§ååˆ†**: é€šå¸¸ã® `dataset[i]` ã§ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
- âœ… **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„**: ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã¯ zarr.Array ã¸ã®å‚ç…§ã®ã¿ï¼ˆè»½é‡ï¼‰
- âœ… **è¿½åŠ å®Ÿè£…ä¸è¦**: `IterableSLDataset.from_zarr()` ã¯å¿…é ˆã§ã¯ãªã„

**metrics_prototypeã§ã®æ´»ç”¨æ–¹æ³•**:

```python
# æ—¢å­˜ã®SLDatasetã§å®Ÿç¾å¯èƒ½
root = zarr.open_group(zarr_path, mode="r")
dataset = SLDataset.from_zarr(root)

# é€šå¸¸ã®Datasetãƒ‘ã‚¿ãƒ¼ãƒ³ã§ä½¿ç”¨
for i in range(len(dataset)):
    item = dataset[i]  # zarr.Arrayå‚ç…§ã‚’å–å¾—
    
    # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ãªæ™‚ç‚¹ã§NumPyå¤‰æ›
    landmarks_np = np.array(item.landmarks["mediapipe.pose"])  # â† ã“ã“ã§åˆã‚ã¦èª­ã¿è¾¼ã¿
    metric.calculate(landmarks_np)
```

---

### 2. âœ… **è¨‚æ­£**: IterableSLDataset.from_zarr() ã«ã¤ã„ã¦

**ç¾çŠ¶**:
- `SLDataset.from_zarr()` ã¯å­˜åœ¨ âœ…
- `IterableSLDataset.from_zarr()` ã¯å­˜åœ¨ã—ãªã„

**åˆ¤æ–­**:
- âŒ **ä¸è¦**: zarr.Array ã¯æ—¢ã«é…å»¶è©•ä¾¡ãªã®ã§ã€è¿½åŠ ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã¯ä¸è¦
- âœ… **æ—¢å­˜ã§ååˆ†**: `SLDataset[i]` ã§ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã‹ã‚‰ zarr.Array å‚ç…§ã‚’å–å¾—
- âœ… **ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰**: å®Ÿãƒ‡ãƒ¼ã‚¿ã¯ `np.array()` æ™‚ã«åˆã‚ã¦èª­ã¿è¾¼ã¾ã‚Œã‚‹

**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦‹ç©ã‚‚ã‚Š**:

```python
# 40,000ã‚¢ã‚¤ãƒ†ãƒ ã®å ´åˆ
# å„ã‚¢ã‚¤ãƒ†ãƒ  = ZarrSLDatasetItem (zarr.Arrayå‚ç…§ã®ã¿)
# â†’ landmarks: {"mediapipe.pose": zarr.Array, "left_hand": zarr.Array, ...}
# â†’ å‚ç…§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚µã‚¤ã‚º: ~æ•°KB/ã‚¢ã‚¤ãƒ†ãƒ 

# 40,000 Ã— æ•°KB = æ•°åã€œæ•°ç™¾MBï¼ˆç®¡ç†å¯èƒ½ï¼‰
# vs å®Ÿãƒ‡ãƒ¼ã‚¿: 40,000 Ã— (300ãƒ•ãƒ¬ãƒ¼ãƒ  Ã— 33ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ Ã— 4æ¬¡å…ƒ Ã— 4bytes) â‰ˆ 6GB
```

**çµè«–**: `IterableSLDataset.from_zarr()` ã®å®Ÿè£…ã¯**ä¸è¦**

---

### 3. âœ… **è¨‚æ­£**: NumPyå¤‰æ›ã¯æ‹¡å¼µæ©Ÿèƒ½ã®è²¬ä»»

**ç¾çŠ¶ã®SLDatasetè¨­è¨ˆ**:

```python
item = dataset[0]
item.landmarks["mediapipe.pose"]  # zarr.Array

# NumPyå¤‰æ›ã¯å¿…è¦ã«å¿œã˜ã¦æ‰‹å‹•ã§è¡Œã†
landmarks_np = np.array(item.landmarks["mediapipe.pose"])
```

**è¨­è¨ˆæ€æƒ³**:
- âœ… **ã‚³ã‚¢ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«**: SLDatasetItemã¯å‹å¤‰æ›ã®è²¬ä»»ã‚’æŒãŸãªã„
- âœ… **æ‹¡å¼µã§å¯¾å¿œ**: å¿…è¦ãªå¤‰æ›ã¯ Transform ã‚„å¾Œå‡¦ç†ã§å®Ÿæ–½
- âœ… **æ—¢å­˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³**: `.to(device)` ã¯PyTorchå°‚ç”¨ï¼ˆå‹å¤‰æ›ã§ã¯ãªãç§»å‹•ï¼‰

**metrics_prototypeã§ã®å¯¾å¿œ**:

```python
# ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: æ‰‹å‹•å¤‰æ›ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
item = dataset[i]
for key, zarr_array in item.landmarks.items():
    landmarks_np = np.array(zarr_array, dtype=np.float32)
    metric.calculate(landmarks_np)

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’ metrics_prototype å´ã§å®šç¾©
def to_numpy_landmarks(
    landmarks: Mapping[str, zarr.Array],
    dtype: np.dtype = np.float32
) -> dict[str, np.ndarray]:
    """Convert zarr.Array landmarks to NumPy arrays."""
    return {
        key: np.array(array, dtype=dtype)
        for key, array in landmarks.items()
    }
```

**çµè«–**: `SLDatasetItem.as_numpy()` ã®è¿½åŠ ã¯**ä¸è¦**ï¼ˆè¨­è¨ˆæ€æƒ³ã«åã™ã‚‹ï¼‰

---

### 4. âš ï¸ **MEDIUM**: ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆå‡¦ç†ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆmetrics_prototypeå´ã§å®Ÿè£…ï¼‰

**metrics_prototypeã®ç¾åœ¨ã®å®Ÿè£…**:

```python
# demo.pyã§æ‰‹å‹•ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
part_categories: dict[str, list[str]] = {
    "Pose": [],
    "L-Hand": [],
    "R-Hand": [],
}

for key in landmarks_dict.keys():
    key_lower = key.lower()
    if "pose" in key_lower:
        part_categories["Pose"].append(key)
    elif "left" in key_lower or "l_hand" in key_lower:
        part_categories["L-Hand"].append(key)
    elif "right" in key_lower or "r_hand" in key_lower:
        part_categories["R-Hand"].append(key)
```

**åˆ¤æ–­**:
- âŒ **sldatasetã«è¿½åŠ ã™ã¹ãã§ã¯ãªã„**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ç‰¹æœ‰ã®ãƒ­ã‚¸ãƒƒã‚¯
- âœ… **metrics_prototypeå´ã§å®Ÿè£…**: ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¯ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ä¾å­˜
- âœ… **å°†æ¥çš„ã«ã¯ metrics ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…**: `metrics/utils.py` ã«é…ç½®

**æ¨å¥¨å®Ÿè£…å ´æ‰€**:

```python
# metrics_prototype/utils.py ï¼ˆæ–°è¦ä½œæˆï¼‰
def categorize_landmarks(
    landmark_keys: Iterable[str]
) -> dict[str, list[str]]:
    """Categorize landmark keys by body part.
    
    Recognizes:
        - Pose: Full body keypoints
        - Left Hand: Left hand keypoints
        - Right Hand: Right hand keypoints
        - Face: Facial landmarks (if present)
    
    Args:
        landmark_keys: Iterator of landmark key names
    
    Returns:
        Dictionary mapping category names to lists of keys
    
    Example:
        >>> keys = ["mediapipe.pose", "mediapipe.left_hand"]
        >>> categorize_landmarks(keys)
        {"Pose": ["mediapipe.pose"], "Left Hand": ["mediapipe.left_hand"]}
    """
    categories: dict[str, list[str]] = {
        "Pose": [],
        "Left Hand": [],
        "Right Hand": [],
        "Face": [],
    }
    
    for key in landmark_keys:
        key_lower = key.lower()
        if "pose" in key_lower or "body" in key_lower:
            categories["Pose"].append(key)
        elif "left" in key_lower and ("hand" in key_lower or "l_hand" in key_lower):
            categories["Left Hand"].append(key)
        elif "right" in key_lower and ("hand" in key_lower or "r_hand" in key_lower):
            categories["Right Hand"].append(key)
        elif "face" in key_lower or "facial" in key_lower:
            categories["Face"].append(key)
    
    return {k: v for k, v in categories.items() if v}  # ç©ºã®ã‚«ãƒ†ã‚´ãƒªã‚’é™¤å¤–


def combine_landmarks(
    landmarks: Mapping[str, np.ndarray],
    keys: list[str],
    axis: int = 1
) -> np.ndarray:
    """Combine multiple landmark arrays along specified axis.
    
    Args:
        landmarks: Mapping of landmark key to array
        keys: Keys to combine
        axis: Axis to concatenate along (default: 1 for keypoints)
    
    Returns:
        Combined NumPy array
    
    Example:
        >>> landmarks = {
        ...     "mediapipe.left_hand": np.zeros((300, 21, 3)),
        ...     "mediapipe.right_hand": np.zeros((300, 21, 3))
        ... }
        >>> combined = combine_landmarks(
        ...     landmarks, ["mediapipe.left_hand", "mediapipe.right_hand"]
        ... )
        >>> combined.shape
        (300, 42, 3)
    """
    arrays = [landmarks[key] for key in keys if key in landmarks]
    if not arrays:
        raise ValueError(f"No valid keys found in landmarks: {keys}")
    return np.concatenate(arrays, axis=axis)
```

**ç”¨é€”**:
```python
from metrics_prototype.utils import categorize_landmarks, combine_landmarks

# ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
categories = categorize_landmarks(item.landmarks.keys())

# ä¸¡æ‰‹çµåˆ
hands_keys = categories["Left Hand"] + categories["Right Hand"]
hands_combined = combine_landmarks(
    {k: np.array(v) for k, v in item.landmarks.items()},
    hands_keys
)
```

---

### 5. âœ… **è¨‚æ­£**: å‹ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯æ—¢ã«å­˜åœ¨

**æ—¢å­˜ã®å‹å®šç¾©**:

```python
# src/cslrtools2/sldataset/dataset/item.py line 43-54
type DefaultSLDatasetItem[Kvid: str, Klm: str, Ktgt: str] = SLDatasetItem[
    Kvid, Any, Klm, Any, Ktgt, Any
]

type TensorSLDatasetItem[Kvid: str, Klm: str, Ktgt: str] = SLDatasetItem[
    Kvid, Tensor, Klm, Tensor, Ktgt, Tensor
]

type ZarrSLDatasetItem[Kvid: str, Klm: str, Ktgt: str] = SLDatasetItem[
    Kvid, zarr.Array, Klm, zarr.Array, Ktgt, zarr.Array
]
```

**åˆ¤æ–­**:
- âœ… **æ—¢ã«å­˜åœ¨**: `DefaultSLDatasetItem[Kvid, Klm, Ktgt]` ãŒ Any å‹ï¼ˆNumPyå«ã‚€ï¼‰
- âœ… **ååˆ†**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã§ã¯ `DefaultSLDatasetItem` ã‚’ä½¿ç”¨ã™ã‚Œã°è‰¯ã„
- âŒ **ä¸è¦**: `NumpySLDatasetItem` ã¨ã„ã†å°‚ç”¨å‹ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯å†—é•·

**metrics_prototypeã§ã®ä½¿ç”¨**:

```python
from cslrtools2.sldataset import DefaultSLDatasetItem

# DefaultSLDatasetItem ã¯ Anyå‹ãªã®ã§ã€np.ndarray ã‚‚å—ã‘å…¥ã‚Œã‚‹
def calculate_metric(
    landmarks: Mapping[str, Any]  # zarr.Array ã¾ãŸã¯ np.ndarray
) -> dict[str, float]:
    # å¿…è¦ã«å¿œã˜ã¦NumPyå¤‰æ›
    for key, array in landmarks.items():
        landmarks_np = np.array(array) if not isinstance(array, np.ndarray) else array
        nan_rate = np.isnan(landmarks_np).any(axis=(1, 2)).mean()
```

---

## ğŸ“Š ä¿®æ­£å¾Œã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³

### âœ… ä¸è¦ãªé …ç›®ï¼ˆèª¤è§£ã‚’è¨‚æ­£ï¼‰

1. âŒ **IterableSLDataset.from_zarr()**: zarr.Array ã¯æ—¢ã«é…å»¶è©•ä¾¡ãªã®ã§ä¸è¦
2. âŒ **SLDataset ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ**: ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆã¯å‚ç…§ã®ã¿ã§è»½é‡
3. âŒ **SLDatasetItem.as_numpy()**: æ‹¡å¼µæ©Ÿèƒ½ã®è²¬ä»»ã€ã‚³ã‚¢ã«ã¯å«ã‚ãªã„
4. âŒ **NumpySLDatasetItemå‹**: DefaultSLDatasetItem ã§ååˆ†

### âš ï¸ å®Ÿè£…ã™ã¹ãé …ç›®ï¼ˆmetrics_prototypeå´ï¼‰

#### Phase 1: ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆå‡¦ç†ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `metrics_prototype/utils.py` ï¼ˆæ–°è¦ä½œæˆï¼‰

**è¿½åŠ é–¢æ•°**:
- `categorize_landmarks()`: ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚­ãƒ¼ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
- `combine_landmarks()`: è¤‡æ•°ãƒ‘ãƒ¼ãƒˆã®çµåˆ
- `to_numpy_landmarks()`: zarr.Array â†’ np.ndarray ä¸€æ‹¬å¤‰æ›

**æ¨å®šä½œæ¥­æ™‚é–“**: 2æ™‚é–“  
**å„ªå…ˆåº¦**: HIGHï¼ˆç¾åœ¨ã®demo.pyã®é‡è¤‡ã‚³ãƒ¼ãƒ‰ã‚’å‰Šæ¸›ï¼‰

---

## ğŸ”„ metrics_prototypeã§ã®æ´»ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¿®æ­£ç‰ˆï¼‰

### Before: ç¾åœ¨ã®å®Ÿè£…

```python
# demo.py - ç›´æ¥zarrã‚¢ã‚¯ã‚»ã‚¹
def calculate_all_samples_metrics(zarr_path: Path) -> None:
    root = zarr.open_group(str(actual_zarr_path), mode="r")
    items_group = root["items"]
    
    i = 0
    while True:
        try:
            item_group = items_group[str(i)]
            landmarks_group = item_group["landmarks"]
            
            # æ‰‹å‹•ã§ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯èª­ã¿è¾¼ã¿
            for key in landmarks_group.array_keys():
                landmark_array = landmarks_group[key]
                landmarks = np.array(landmark_array).astype(np.float32)
                # ...
```

### After: SLDatasetçµ±åˆå¾Œï¼ˆä¿®æ­£ç‰ˆï¼‰

```python
# demo.py (ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ)
from cslrtools2.sldataset import SLDataset
from metrics_prototype.utils import (
    categorize_landmarks,
    combine_landmarks,
    to_numpy_landmarks
)

def calculate_all_samples_metrics(zarr_path: Path) -> None:
    # SLDatasetä½¿ç”¨ï¼ˆzarr.Arrayå‚ç…§ã®ã¿èª­ã¿è¾¼ã¿ï¼‰
    root = zarr.open_group(str(zarr_path), mode="r")
    dataset = SLDataset.from_zarr(root)
    
    # å„ã‚µãƒ³ãƒ—ãƒ«ã‚’å‡¦ç†
    for i in range(len(dataset)):
        if i % 1000 == 0:
            print(f"  Processing sample {i}...")
        
        item = dataset[i]  # ZarrSLDatasetItem (å‚ç…§ã®ã¿)
        
        # NumPyå¤‰æ›ï¼ˆå¿…è¦ãªæ™‚ç‚¹ã§å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼‰
        landmarks_np = to_numpy_landmarks(item.landmarks, dtype=np.float32)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
        categories = categorize_landmarks(landmarks_np.keys())
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        for category, keys in categories.items():
            if not keys:
                continue
            
            # å˜ä¸€ãƒ‘ãƒ¼ãƒˆ
            if len(keys) == 1:
                landmarks = landmarks_np[keys[0]]
            # è¤‡æ•°ãƒ‘ãƒ¼ãƒˆçµåˆï¼ˆä¾‹: ä¸¡æ‰‹ï¼‰
            else:
                landmarks = combine_landmarks(landmarks_np, keys)
            
            result = metric_nan.calculate(landmarks)
            all_results[category].append(result["values"]["nan_rate"])
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- âœ… ã‚³ãƒ¼ãƒ‰é‡ç´„30%å‰Šæ¸›ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒ `get_group()` ã«é›†ç´„ï¼‰
- âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ¨™æº–åŒ–ï¼ˆ`DataLoadError`ï¼‰
- âœ… å‹å®‰å…¨æ€§ã®å‘ä¸Šï¼ˆ`ZarrSLDatasetItem`ï¼‰
- âœ… ä¿å®ˆæ€§ã®å‘ä¸Šï¼ˆsldatasetã¨ã®ä¸€è²«æ€§ï¼‰

**é‡è¦ãªç‚¹**:
- âœ… **é…å»¶è©•ä¾¡**: `dataset[i]` ã¯ zarr.Array å‚ç…§ã®ã¿å–å¾—ï¼ˆè»½é‡ï¼‰
- âœ… **ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰**: `to_numpy_landmarks()` å†…ã® `np.array()` ã§å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
- âœ… **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: ç¾åœ¨ã®æœ€é©åŒ–ç‰ˆã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

---

## ğŸ§ª æ¤œè¨¼è¨ˆç”»

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ãƒŠãƒªã‚ª

```python
import time
import tracemalloc
from cslrtools2.sldataset import SLDataset, IterableSLDataset

def benchmark_loading_strategies(zarr_path: str):
    """Compare different loading strategies."""
    
    # Strategy 1: SLDataset.from_zarr() (å…¨èª­ã¿è¾¼ã¿)
    print("Strategy 1: SLDataset.from_zarr()")
    tracemalloc.start()
    start = time.time()
    
    root = zarr.open_group(zarr_path, mode="r")
    dataset = SLDataset.from_zarr(root)
    
    elapsed = time.time() - start
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    print(f"  Time: {elapsed:.2f}s")
    print(f"  Memory: {peak / 1024 / 1024:.2f}MB")
    print(f"  Items: {len(dataset)}")
    
    # Strategy 2: IterableSLDataset.from_zarr() (é…å»¶èª­ã¿è¾¼ã¿)
    print("\nStrategy 2: IterableSLDataset.from_zarr()")
    tracemalloc.start()
    start = time.time()
    
    root = zarr.open_group(zarr_path, mode="r")
    iterable_dataset = IterableSLDataset.from_zarr(root)
    
    # æœ€åˆã®1000ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆ
    for i, item in enumerate(iterable_dataset):
        if i >= 1000:
            break
    
    elapsed = time.time() - start
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    print(f"  Time (first 1000): {elapsed:.2f}s")
    print(f"  Memory: {peak / 1024 / 1024:.2f}MB")
    
    # Strategy 3: metrics_prototypeæœ€é©åŒ–ç‰ˆ (ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹)
    print("\nStrategy 3: Direct zarr access (current metrics_prototype)")
    tracemalloc.start()
    start = time.time()
    
    root = zarr.open_group(zarr_path, mode="r")
    items_group = root["items"]
    
    for i in range(1000):
        item_group = items_group[str(i)]
        # ... process
    
    elapsed = time.time() - start
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    print(f"  Time (first 1000): {elapsed:.2f}s")
    print(f"  Memory: {peak / 1024 / 1024:.2f}MB")
```

**æœŸå¾…çµæœ**:
- Strategy 1: ãƒ¡ãƒ¢ãƒªå¤§ã€åˆæœŸåŒ–é…ã„ã€ã‚¢ã‚¯ã‚»ã‚¹é«˜é€Ÿ
- Strategy 2: ãƒ¡ãƒ¢ãƒªå°ã€åˆæœŸåŒ–é«˜é€Ÿã€ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­é€Ÿ
- Strategy 3: ãƒ¡ãƒ¢ãƒªå°ã€åˆæœŸåŒ–é«˜é€Ÿã€ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é«˜é€Ÿ

**åˆ¤æ–­åŸºæº–**:
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ < 4GBï¼ˆ40,000ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- 1000ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†æ™‚é–“ < 30ç§’
- å…¨ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†æ™‚é–“ < 10åˆ†

---

## ğŸ“ ã¾ã¨ã‚ï¼ˆä¿®æ­£ç‰ˆï¼‰

### âœ… èª¤è§£ã‚’è¨‚æ­£ã—ãŸçµæœ

#### ä¸è¦ãªå®Ÿè£…ï¼ˆsldatasetå´ï¼‰
1. âŒ `IterableSLDataset.from_zarr()`: zarr.Array ã¯é…å»¶è©•ä¾¡æ¸ˆã¿
2. âŒ `SLDataset.from_zarr()` ã®å¤‰æ›´: æ—¢ã«åŠ¹ç‡çš„
3. âŒ `SLDatasetItem.as_numpy()`: æ‹¡å¼µæ©Ÿèƒ½ã®è²¬ä»»
4. âŒ `NumpySLDatasetItem` å‹: DefaultSLDatasetItem ã§ååˆ†

#### å¿…è¦ãªå®Ÿè£…ï¼ˆmetrics_prototypeå´ï¼‰

### ğŸŸ¡ HIGHï¼ˆ1é€±é–“ä»¥å†…ï¼‰
1. âœ… `metrics_prototype/utils.py` ã®æ–°è¦ä½œæˆ
   - `categorize_landmarks()`: ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
   - `combine_landmarks()`: ãƒ‘ãƒ¼ãƒˆçµåˆ
   - `to_numpy_landmarks()`: zarr â†’ NumPyå¤‰æ›

### ğŸŸ¢ MEDIUMï¼ˆ2é€±é–“ä»¥å†…ï¼‰
2. âœ… `demo.py` ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
   - SLDataset.from_zarr() ä½¿ç”¨ã«åˆ‡ã‚Šæ›¿ãˆ
   - utils.py ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°æ´»ç”¨
3. âœ… ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆè¿½åŠ 
   - utils.py ã®é–¢æ•°ãƒ†ã‚¹ãƒˆ

### å®Ÿè£…å¾Œã®åŠ¹æœï¼ˆä¿®æ­£ç‰ˆï¼‰
- âœ… metrics_prototypeã®ã‚³ãƒ¼ãƒ‰é‡ç´„**30%å‰Šæ¸›**
- âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ¨™æº–åŒ–ï¼ˆDataLoadErrorï¼‰
- âœ… å‹å®‰å…¨æ€§ã®å‘ä¸Šï¼ˆZarrSLDatasetItemï¼‰
- âœ… ä¿å®ˆæ€§ã®å‘ä¸Šï¼ˆsldatasetã¨ã®ä¸€è²«æ€§ï¼‰
- âœ… **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ç¾çŠ¶ç¶­æŒ**ï¼ˆé…å»¶è©•ä¾¡ã®ãŸã‚ï¼‰

---

## ğŸ”— é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è§£èª¬**: `docs/METRICS_PROTOTYPE_ARCHITECTURE.md`
- **ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ææ¡ˆ**: `METRICS_PROTOTYPE_REFACTORING_PROPOSAL.md`
- **ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**: `SLDATASET_REFACTORING_CHECKLIST.md`
