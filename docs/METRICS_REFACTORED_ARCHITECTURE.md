# Metrics System ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**ä½œæˆæ—¥**: 2025-12-03  
**åŸºæº–**: METRICS_IMPLEMENTATION_DRAFT.md + metrics_prototype2/ + evaluate_real_dataset_v2.py  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: è¨­è¨ˆç¢ºå®šç‰ˆï¼ˆå®Ÿè£…ã‚¬ã‚¤ãƒ‰ï¼‰

---

## ğŸ“‹ ç›®æ¬¡

1. [è¨­è¨ˆåŸå‰‡](#è¨­è¨ˆåŸå‰‡)
2. [ã‚¯ãƒ©ã‚¹éšå±¤å…¨ä½“å›³](#ã‚¯ãƒ©ã‚¹éšå±¤å…¨ä½“å›³)
3. [ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ](#ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ)
4. [ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
5. [ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ](#ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ )
6. [é›†ç´„ã‚·ã‚¹ãƒ†ãƒ ](#é›†ç´„ã‚·ã‚¹ãƒ†ãƒ )
7. [ä¸¦åˆ—å‡¦ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#ä¸¦åˆ—å‡¦ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
8. [ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ](#ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ)
9. [å®Ÿè£…ä¾‹](#å®Ÿè£…ä¾‹)

---

## è¨­è¨ˆåŸå‰‡

### 1. è²¬ä»»ã®æ˜ç¢ºãªåˆ†é›¢ï¼ˆSeparation of Concernsï¼‰

```mermaid
graph LR
    subgraph "Data Layer"
        DATASET[SLDataset]
        ITEM[SLDatasetItem]
        LANDMARKS[landmarks dict]
    end
    
    subgraph "Extraction Layer"
        CALC[MetricCalculator]
        FILTER[_filter]
        RESOLVE[_resolve_categories]
    end
    
    subgraph "Computation Layer"
        METRIC[Metric]
        CALC_METHOD[calculate]
    end
    
    subgraph "Aggregation Layer"
        AGG[Aggregator]
        INIT[init]
        UPDATE[update]
        FINAL[finalize]
    end
    
    subgraph "Plugin Layer"
        LOADER[MetricLoader]
        EP[Entry Points]
        VALIDATE[validate_plugin]
    end
    
    DATASET --> CALC
    CALC --> FILTER
    FILTER --> LANDMARKS
    LANDMARKS --> METRIC
    METRIC --> CALC_METHOD
    CALC_METHOD --> AGG
    AGG --> INIT
    AGG --> UPDATE
    AGG --> FINAL
    
    EP --> LOADER
    LOADER --> VALIDATE
    VALIDATE --> METRIC
    
    style CALC fill:#e1f5ff
    style METRIC fill:#fff4e1
    style AGG fill:#e8f5e9
    style LOADER fill:#fce4ec
```

**å„å±¤ã®è²¬å‹™**:

| å±¤ | è²¬å‹™ | æ‹…å½“ã‚¯ãƒ©ã‚¹ |
|----|------|-----------|
| Data | ãƒ‡ãƒ¼ã‚¿ä¿æŒãƒ»ã‚¢ã‚¯ã‚»ã‚¹ | `SLDataset`, `SLDatasetItem` |
| Extraction | ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»ã‚«ãƒ†ã‚´ãƒªãƒ¼è§£æ±º | `MetricCalculator._filter()`, `_resolve_categories()` |
| Computation | ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆç´”ç²‹é–¢æ•°ï¼‰ | `Metric.calculate()` |
| Aggregation | çµæœé›†ç´„ãƒ»çµ±è¨ˆè¨ˆç®— | `Aggregator.init/update/finalize()` |
| Plugin | ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ç™ºè¦‹ãƒ»æ¤œè¨¼ | `MetricLoader`, Entry Points |

---

## ã‚¯ãƒ©ã‚¹éšå±¤å…¨ä½“å›³

```mermaid
classDiagram
    %% å‹å®šç¾©
    class MetricResult {
        <<TypedDict>>
        +str metric_name
        +dict[str, float] values
        +dict[str, Any] metadata
        +str|int sample_id
    }
    
    class CalculationSpec {
        <<Pydantic>>
        +str method
        +list~Category~ categories
        +ConfigDict model_config
        +...kwargs Any
    }
    
    %% æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹
    class Metric {
        <<abstract>>
        +calculate(data: NDArray) MetricResult
        +get_description() str
        +validate_inputs(data: NDArray) bool
    }
    
    class LandmarkMetric {
        <<abstract>>
        +calculate(data: NDArray) MetricResult
        +validate_inputs(data: NDArray) bool
        +get_description() str
    }
    
    class Aggregator~T~ {
        <<Generic>>
        +init() T
        +update(state: T, result: MetricResult) T
        +finalize(state: T) dict[str, float]
    }
    
    %% å…·ä½“å®Ÿè£…
    class NaNRateMetric {
        +calculate(data: NDArray) MetricResult
        +get_description() str
        -_calculate_frame_level_nan_rate(data) float
    }
    
    class TemporalConsistencyMetric {
        +window_size: int
        +calculate(data: NDArray) MetricResult
        +get_description() str
        -_calculate_smoothness(data) float
    }
    
    class AnatomicalConstraintMetric {
        +bone_pairs: list[tuple[int, int]]
        +calculate(data: NDArray) MetricResult
        +get_description() str
        -_calculate_bone_length_variation(data) float
    }
    
    class DefaultAggregator {
        +init() list[MetricResult]
        +update(state, result) list[MetricResult]
        +finalize(state) dict[str, float]
        -_compute_statistics(values) dict
    }
    
    class StreamingAggregator {
        +init() AggregatorState
        +update(state, result) AggregatorState
        +finalize(state) dict[str, float]
    }
    
    %% ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
    class MetricCalculator {
        -specs: list[CalculationSpec]
        -category_definitions: dict
        -metrics: dict[str, Metric]
        -aggregators: dict[str, Aggregator]
        +__init__(specs, category_defs)
        +run(dataset: SLDataset, workers: int) dict
        -_filter(item, category_spec) NDArray
        -_resolve_categories(spec) list[str]
        -_process_sample(dataset, idx) list[MetricResult]
        -_aggregate_results(results) dict
    }
    
    %% ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ 
    class MetricLoader {
        -plugins: dict[str, type[Metric]]
        +load_plugins() dict
        +validate_plugin(cls: type) bool
        +get_metric_class(name: str) type[Metric]
        +list_available_metrics() list[str]
    }
    
    class MetricInfo {
        <<TypedDict>>
        +str name
        +str category
        +str metric_name
        +type[Metric] metric_class
        +dict default_params
    }
    
    %% é–¢ä¿‚
    Metric <|-- LandmarkMetric : extends
    LandmarkMetric <|-- NaNRateMetric : implements
    LandmarkMetric <|-- TemporalConsistencyMetric : implements
    LandmarkMetric <|-- AnatomicalConstraintMetric : implements
    
    Aggregator <|-- DefaultAggregator : implements
    Aggregator <|-- StreamingAggregator : implements
    
    Metric ..> MetricResult : creates
    Aggregator ..> MetricResult : aggregates
    
    MetricCalculator --> CalculationSpec : uses
    MetricCalculator --> Metric : instantiates
    MetricCalculator --> Aggregator : uses
    MetricCalculator ..> MetricResult : produces
    
    MetricLoader --> Metric : discovers
    MetricLoader --> MetricInfo : provides
    MetricCalculator --> MetricLoader : uses
    
    note for Metric "ç´”ç²‹é–¢æ•°\n1ã‚µãƒ³ãƒ—ãƒ« â†’ 1çµæœ"
    note for Aggregator "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é›†ç´„\nãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–"
    note for MetricCalculator "ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\nä¸¦åˆ—å‡¦ç†ç®¡ç†"
```

---

## ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### 1. MetricResultï¼ˆå‹å®šç¾©ï¼‰

**æ±ºå®šäº‹é …**: TypedDictï¼ˆQ23ï¼‰

```mermaid
graph TB
    subgraph "MetricResult Structure"
        MR[MetricResult: TypedDict]
        NAME[metric_name: str]
        VALUES[values: dict str, float]
        META[metadata: dict str, Any]
        SAMPLE[sample_id: str or int]
        
        MR --> NAME
        MR --> VALUES
        MR --> META
        MR --> SAMPLE
    end
    
    subgraph "Values Example"
        V1[nan_rate: 0.05]
        V2[frames_with_nan: 15]
        V3[smoothness: 0.03]
    end
    
    subgraph "Metadata Example"
        M1[total_frames: 300]
        M2[shape: tuple~300, 33, 3~]
        M3[engine: 'mediapipe']
    end
    
    VALUES --> V1
    VALUES --> V2
    VALUES --> V3
    
    META --> M1
    META --> M2
    META --> M3
    
    style MR fill:#e1f5ff
```

**å®Ÿè£…**:

```python
from typing import TypedDict, Any

class MetricResult(TypedDict):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—çµæœï¼ˆTypedDictç‰ˆï¼‰
    
    é¸æŠç†ç”±:
    - æœ€è»½é‡ï¼ˆ40K+ã‚µãƒ³ãƒ—ãƒ«ã®é›†ç´„ã§æœ‰åˆ©ï¼‰
    - é™çš„å‹ãƒã‚§ãƒƒã‚¯ï¼ˆPyrightï¼‰ã§å‹å®‰å…¨æ€§ç¢ºä¿
    - è¾æ›¸äº’æ›æ€§ï¼ˆJSONå¤‰æ›å®¹æ˜“ï¼‰
    """
    metric_name: str
    values: dict[str, float]
    metadata: dict[str, Any]
    sample_id: str | int
```

---

### 2. MetricåŸºåº•ã‚¯ãƒ©ã‚¹

**æ±ºå®šäº‹é …**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIã¯å«ã‚ãªã„ï¼ˆQ27ï¼‰

```mermaid
classDiagram
    class Metric {
        <<abstract>>
        +calculate(data: NDArray, **kwargs) MetricResult*
        +get_description() str*
        +validate_inputs(data: NDArray) bool
        +__repr__() str
    }
    
    class LandmarkMetric {
        <<abstract>>
        +calculate(data: NDArray[float32], **kwargs) MetricResult*
        +validate_inputs(data: NDArray) bool
        +get_description() str*
        -_validate_shape(data) bool
    }
    
    Metric <|-- LandmarkMetric
    
    note for Metric "å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åŸºåº•\nã‚¨ãƒ³ã‚¸ãƒ³éä¾å­˜"
    note for LandmarkMetric "ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯å°‚ç”¨\nå½¢çŠ¶æ¤œè¨¼å«ã‚€"
```

**å®Ÿè£…**:

```python
from abc import ABC, abstractmethod
from typing import Any
import numpy as np
from numpy.typing import NDArray

class Metric(ABC):
    """å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹
    
    è¨­è¨ˆåŸå‰‡:
    - ç´”ç²‹é–¢æ•°: 1ã‚µãƒ³ãƒ—ãƒ« â†’ 1çµæœ
    - ã‚¨ãƒ³ã‚¸ãƒ³éä¾å­˜: NumPyé…åˆ—ã§å‹•ä½œ
    - Ground Truthä¸è¦
    """
    
    @abstractmethod
    def calculate(self, data: Any, **kwargs: Any) -> MetricResult:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        
        Args:
            data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆå‹ã¯ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®šç¾©ï¼‰
            **kwargs: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            è¨ˆç®—çµæœï¼ˆMetricResultï¼‰
        """
        pass
    
    @abstractmethod
    def get_description(self) -> str:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®èª¬æ˜ã‚’è¿”ã™"""
        pass
    
    def validate_inputs(self, data: Any) -> bool:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
        return True
    
    def __repr__(self) -> str:
        return f"{self.__class__.__name__}()"


class LandmarkMetric(Metric):
    """ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åŸºåº•ã‚¯ãƒ©ã‚¹
    
    æœŸå¾…ã™ã‚‹å…¥åŠ›å½¢çŠ¶: (frames, keypoints, coordinates)
    - frames: ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆTï¼‰
    - keypoints: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°ï¼ˆKï¼‰ã€ä¾‹: MediaPipe Poseã§33
    - coordinates: åº§æ¨™æ¬¡å…ƒï¼ˆDï¼‰ã€é€šå¸¸3ï¼ˆx, y, zï¼‰ã¾ãŸã¯4ï¼ˆx, y, z, visibilityï¼‰
    """
    
    @abstractmethod
    def calculate(self, data: NDArray[np.float32], **kwargs: Any) -> MetricResult:
        """ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        
        Args:
            data: shape (T, K, D) ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯é…åˆ—
            **kwargs: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            è¨ˆç®—çµæœ
        """
        pass
    
    def validate_inputs(self, data: NDArray[np.float32]) -> bool:
        """3æ¬¡å…ƒé…åˆ—ã‹ã¤ãƒ•ãƒ¬ãƒ¼ãƒ æ•° >= 1 ã‚’æ¤œè¨¼"""
        if data.ndim != 3:
            return False
        
        frames, keypoints, coords = data.shape
        return frames >= 1 and keypoints >= 1 and coords >= 1
    
    @abstractmethod
    def get_description(self) -> str:
        pass
```

---

### 3. Aggregatorï¼ˆé›†ç´„ã‚·ã‚¹ãƒ†ãƒ ï¼‰

**æ±ºå®šäº‹é …**: Metricã¨ã¯åˆ†é›¢ï¼ˆQ27ï¼‰

```mermaid
stateDiagram-v2
    [*] --> åˆæœŸåŒ–
    åˆæœŸåŒ– --> é›†ç´„ä¸­
    
    é›†ç´„ä¸­ --> é›†ç´„ä¸­: update state
    
    note right of é›†ç´„ä¸­
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
        update(state, result)
        ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„
        40K+ã‚µãƒ³ãƒ—ãƒ«å¯¾å¿œ
    end note
    
    é›†ç´„ä¸­ --> æœ€çµ‚åŒ–: å…¨ã‚µãƒ³ãƒ—ãƒ«å®Œäº†
    æœ€çµ‚åŒ– --> [*]
    
    note right of æœ€çµ‚åŒ–
        finalize(state)
        mean, std, min, max
        samples ã‚’è¨ˆç®—
    end note
```

**å®Ÿè£…**:

```python
from typing import Generic, TypeVar

T = TypeVar('T')  # é›†ç´„çŠ¶æ…‹ã®å‹

class Aggregator(Generic[T]):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœã®é›†ç´„åŸºåº•ã‚¯ãƒ©ã‚¹
    
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é›†ç´„ã‚’ã‚µãƒãƒ¼ãƒˆ:
    - init(): åˆæœŸçŠ¶æ…‹ã‚’ç”Ÿæˆ
    - update(): 1ã‚µãƒ³ãƒ—ãƒ«çµæœã§çŠ¶æ…‹æ›´æ–°
    - finalize(): æœ€çµ‚é›†ç´„çµæœã‚’è¨ˆç®—
    """
    
    def init(self) -> T:
        """åˆæœŸé›†ç´„çŠ¶æ…‹ã‚’è¿”ã™
        
        Returns:
            åˆæœŸçŠ¶æ…‹ï¼ˆå‹ã¯å®Ÿè£…ã«ã‚ˆã‚‹ï¼‰
        """
        raise NotImplementedError
    
    def update(self, state: T, result: MetricResult) -> T:
        """1ã‚µãƒ³ãƒ—ãƒ«ã®çµæœã§çŠ¶æ…‹ã‚’æ›´æ–°
        
        Args:
            state: ç¾åœ¨ã®é›†ç´„çŠ¶æ…‹
            result: æ–°ã—ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœ
        
        Returns:
            æ›´æ–°å¾Œã®çŠ¶æ…‹
        """
        raise NotImplementedError
    
    def finalize(self, state: T) -> dict[str, float]:
        """æœ€çµ‚é›†ç´„çµæœã‚’è¨ˆç®—
        
        Args:
            state: æœ€çµ‚çŠ¶æ…‹
        
        Returns:
            é›†ç´„çµ±è¨ˆé‡ï¼ˆmean, std, min, max, samplesç­‰ï¼‰
        """
        raise NotImplementedError


class DefaultAggregator(Aggregator[list[MetricResult]]):
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé›†ç´„å®Ÿè£…ï¼ˆãƒªã‚¹ãƒˆè“„ç©å‹ï¼‰
    
    å…¨çµæœã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã—ã€æœ€å¾Œã«çµ±è¨ˆé‡ã‚’è¨ˆç®—ã€‚
    å°ã€œä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‘ã‘ã€‚
    """
    
    def init(self) -> list[MetricResult]:
        return []
    
    def update(self, state: list[MetricResult], result: MetricResult) -> list[MetricResult]:
        state.append(result)
        return state
    
    def finalize(self, state: list[MetricResult]) -> dict[str, float]:
        """mean, std, min, max, samples ã‚’è¨ˆç®—"""
        if not state:
            return {}
        
        # å…¨çµæœã‹ã‚‰å€¤ã‚’æŠ½å‡ºï¼ˆä¾‹: 'nan_rate' ã‚­ãƒ¼ï¼‰
        # å®Ÿéš›ã«ã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã«å¿œã˜ã¦å‹•çš„ã«å‡¦ç†
        values = [r['values'] for r in state]
        
        # NumPyã§çµ±è¨ˆè¨ˆç®—
        import numpy as np
        values_array = np.array([list(v.values())[0] for v in values])
        
        return {
            'mean': float(np.mean(values_array)),
            'std': float(np.std(values_array)),
            'min': float(np.min(values_array)),
            'max': float(np.max(values_array)),
            'samples': len(state)
        }


class StreamingAggregator(Aggregator[dict[str, float]]):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é›†ç´„å®Ÿè£…ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‹ï¼‰
    
    Welfordã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç­‰ã§é€æ¬¡çµ±è¨ˆé‡ã‚’æ›´æ–°ã€‚
    å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ40K+ã‚µãƒ³ãƒ—ãƒ«ï¼‰å‘ã‘ã€‚
    """
    
    def init(self) -> dict[str, float]:
        return {
            'sum': 0.0,
            'sum_sq': 0.0,
            'min': float('inf'),
            'max': float('-inf'),
            'count': 0
        }
    
    def update(self, state: dict[str, float], result: MetricResult) -> dict[str, float]:
        """Welfordã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å¹³å‡ãƒ»åˆ†æ•£ã‚’æ›´æ–°"""
        # çµæœã‹ã‚‰å€¤ã‚’æŠ½å‡ºï¼ˆç°¡ç•¥åŒ–ï¼‰
        value = list(result['values'].values())[0]
        
        return {
            'sum': state['sum'] + value,
            'sum_sq': state['sum_sq'] + value ** 2,
            'min': min(state['min'], value),
            'max': max(state['max'], value),
            'count': state['count'] + 1
        }
    
    def finalize(self, state: dict[str, float]) -> dict[str, float]:
        """æœ€çµ‚çµ±è¨ˆé‡ã‚’è¨ˆç®—"""
        if state['count'] == 0:
            return {}
        
        mean = state['sum'] / state['count']
        variance = (state['sum_sq'] / state['count']) - mean ** 2
        
        return {
            'mean': mean,
            'std': variance ** 0.5,
            'min': state['min'],
            'max': state['max'],
            'samples': int(state['count'])
        }
```

---

## ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### å…¨ä½“ãƒ•ãƒ­ãƒ¼

```mermaid
sequenceDiagram
    participant User
    participant CLI
    participant Calculator
    participant Loader
    participant Dataset
    participant Filter
    participant Metric
    participant Aggregator
    participant Output

    User->>CLI: sldataset metrics calculate --config config.yaml
    CLI->>Calculator: åˆæœŸåŒ–(CalculationSpec[])
    
    Calculator->>Loader: load_plugins()
    Loader-->>Calculator: dict[str, type[Metric]]
    
    Calculator->>Calculator: _resolve_categories()
    Note over Calculator: ã‚«ãƒ†ã‚´ãƒªãƒ¼åå‰è§£æ±º<br/>å¾ªç’°æ¤œå‡º
    
    Calculator->>Aggregator: init()
    Aggregator-->>Calculator: åˆæœŸçŠ¶æ…‹
    
    par ä¸¦åˆ—å‡¦ç†ï¼ˆloky.ProcessPoolExecutorï¼‰
        Calculator->>Dataset: get_item(0)
        Dataset-->>Filter: SLDatasetItem
        Filter->>Filter: _filter(item, category_spec)
        Note over Filter: æ­£è¦è¡¨ç¾ãƒãƒƒãƒãƒ³ã‚°<br/>.*\.category
        Filter->>Filter: np.concatenate(arrays)
        Filter-->>Metric: NDArray (T, K, D)
        
        Metric->>Metric: calculate(data)
        Note over Metric: ç´”ç²‹é–¢æ•°<br/>1ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†
        Metric-->>Aggregator: MetricResult
        
        Calculator->>Dataset: get_item(1)
        Dataset-->>Filter: SLDatasetItem
        Filter-->>Metric: NDArray
        Metric-->>Aggregator: MetricResult
        
        Note over Calculator,Aggregator: 40,214ã‚µãƒ³ãƒ—ãƒ«ä¸¦åˆ—å‡¦ç†...
    end
    
    loop å„çµæœ
        Aggregator->>Aggregator: update(state, result)
    end
    
    Aggregator->>Aggregator: finalize(state)
    Aggregator-->>Calculator: dict[str, float]
    
    Calculator->>Output: JSONæ›¸ãè¾¼ã¿
    Output-->>User: results.json
```

---

### ã‚«ãƒ†ã‚´ãƒªãƒ¼è§£æ±ºãƒ•ãƒ­ãƒ¼

```mermaid
flowchart TD
    START[CalculationSpec.categories]
    
    SINGLE{å˜ä¸€æ–‡å­—åˆ—?}
    LIST{ç„¡åãƒªã‚¹ãƒˆ?}
    DICT{åå‰ä»˜ãè¾æ›¸?}
    
    RESOLVE_STR[æ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä½¿ç”¨]
    RESOLVE_LIST[å„è¦ç´ ã‚’å‡¦ç†]
    RESOLVE_DICT[nameã‚’ã‚­ãƒ¼ã€landmarksã‚’è§£æ±º]
    
    GLOBAL{ã‚°ãƒ­ãƒ¼ãƒãƒ«å®šç¾©<br/>å‚ç…§?}
    LOOKUP[category_definitionsã‹ã‚‰å–å¾—]
    CIRCULAR[å¾ªç’°å‚ç…§ãƒã‚§ãƒƒã‚¯<br/>visited set]
    
    REGEX[æ­£è¦è¡¨ç¾ç”Ÿæˆ: .*\.category]
    MATCH[item.landmarksã‹ã‚‰<br/>ã‚­ãƒ¼ãƒãƒƒãƒãƒ³ã‚°]
    LOAD[zarr Arrayãƒ­ãƒ¼ãƒ‰]
    CONCAT[np.concatenateè¤‡æ•°é…åˆ—]
    
    OUTPUT[NDArray]
    
    START --> SINGLE
    SINGLE -->|Yes| RESOLVE_STR
    SINGLE -->|No| LIST
    
    LIST -->|Yes| RESOLVE_LIST
    LIST -->|No| DICT
    
    DICT -->|Yes| RESOLVE_DICT
    
    RESOLVE_STR --> GLOBAL
    RESOLVE_LIST --> GLOBAL
    RESOLVE_DICT --> GLOBAL
    
    GLOBAL -->|Yes| LOOKUP
    GLOBAL -->|No| REGEX
    
    LOOKUP --> CIRCULAR
    CIRCULAR -->|OK| REGEX
    CIRCULAR -->|NG| ERROR[CircularReferenceError]
    
    REGEX --> MATCH
    MATCH --> LOAD
    LOAD --> CONCAT
    CONCAT --> OUTPUT
    
    style CIRCULAR fill:#fff4e1
    style ERROR fill:#ffebee
    style OUTPUT fill:#e8f5e9
```

---

## ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ 

### Entry Pointsç™ºè¦‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

```mermaid
graph TB
    subgraph "pyproject.toml"
        TOML["[project.entry-points.'cslrtools2.metrics']<br/>NaNRateMetric = 'pkg:NaNRateMetric'"]
    end
    
    subgraph "MetricLoader"
        EP[importlib.metadata.entry_points]
        SELECT[select group='cslrtools2.metrics']
        LOAD[ep.load -> class]
        VALIDATE[validate_plugin]
    end
    
    subgraph "æ¤œè¨¼"
        CHECK1[Metricã‚µãƒ–ã‚¯ãƒ©ã‚¹]
        CHECK2[calculateå­˜åœ¨]
        SKIP["è­¦å‘Šãƒ­ã‚° and ã‚¹ã‚­ãƒƒãƒ—"]
        OK[ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ç™»éŒ²]
    end
    
    subgraph "MetricCalculator"
        CALC[ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–]
    end
    
    TOML --> EP
    EP --> SELECT
    SELECT --> LOAD
    LOAD --> VALIDATE
    
    VALIDATE --> CHECK1
    CHECK1 -->|No| SKIP
    CHECK1 -->|Yes| CHECK2
    CHECK2 -->|No| SKIP
    CHECK2 -->|Yes| OK
    
    OK --> CALC
    
    style VALIDATE fill:#fce4ec
    style OK fill:#e8f5e9
    style SKIP fill:#fff9c4
```

**æ±ºå®šäº‹é …**ï¼ˆQ26ï¼‰:
- æ¤œè¨¼ãƒ¬ãƒ™ãƒ«: ãƒ¬ãƒ™ãƒ«1ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰
- ã‚¿ã‚¤ãƒŸãƒ³ã‚°: ãƒ­ãƒ¼ãƒ‰æ™‚
- ã‚¨ãƒ©ãƒ¼å‡¦ç†: è­¦å‘Šãƒ­ã‚°ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ
- å®Ÿè£…å ´æ‰€: `loader.py`

**å®Ÿè£…**:

```python
# loader.py
import logging
from importlib.metadata import entry_points
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .base import Metric

logger = logging.getLogger(__name__)

def load_metric_plugins() -> dict[str, type['Metric']]:
    """Entry Pointsã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰
    
    Returns:
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹å â†’ Metricã‚¯ãƒ©ã‚¹ã®è¾æ›¸
    """
    from .base import Metric  # å¾ªç’°importå›é¿
    
    metrics: dict[str, type[Metric]] = {}
    
    eps = entry_points(group='cslrtools2.metrics')
    
    for ep in eps:
        try:
            plugin_class = ep.load()
            
            # ãƒ¬ãƒ™ãƒ«1æ¤œè¨¼: ã‚µãƒ–ã‚¯ãƒ©ã‚¹ãƒã‚§ãƒƒã‚¯
            if not issubclass(plugin_class, Metric):
                logger.warning(
                    f"Plugin '{ep.name}' is not a Metric subclass, skipping"
                )
                continue
            
            metrics[ep.name] = plugin_class
            logger.info(f"Loaded metric plugin: {ep.name}")
            
        except Exception as e:
            logger.warning(f"Failed to load plugin '{ep.name}': {e}")
    
    return metrics


def list_available_metrics() -> list[str]:
    """åˆ©ç”¨å¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åä¸€è¦§"""
    return sorted(load_metric_plugins().keys())
```

---

## é›†ç´„ã‚·ã‚¹ãƒ†ãƒ 

### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé›†ç´„ vs ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é›†ç´„

```mermaid
graph TB
    subgraph "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé›†ç´„ï¼ˆå°è¦æ¨¡ï¼‰"
        D1[init: list]
        D2[update: append]
        D3[finalize: np.mean/std]
        
        D1 --> D2
        D2 --> D3
        
        DN[ãƒ¡ãƒ¢ãƒª: O N]
        D3 --> DN
    end
    
    subgraph "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é›†ç´„ï¼ˆå¤§è¦æ¨¡ï¼‰"
        S1[init: dict sum, sum_sq, count]
        S2["update: Welford's algorithm"]
        S3[finalize: è¨ˆç®—å¼ã‹ã‚‰çµ±è¨ˆé‡]
        
        S1 --> S2
        S2 --> S3
        
        SN[ãƒ¡ãƒ¢ãƒª: O 1]
        S3 --> SN
    end
    
    subgraph "ä½¿ã„åˆ†ã‘"
        SMALL[< 10K samples]
        LARGE[>= 10K samples]
        
        SMALL --> D1
        LARGE --> S1
    end
    
    style DN fill:#fff4e1
    style SN fill:#e8f5e9
```

**é¸æŠåŸºæº–**:

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¦æ¨¡ | æ¨å¥¨Aggregator | ç†ç”± |
|-----------------|---------------|------|
| < 10K ã‚µãƒ³ãƒ—ãƒ« | DefaultAggregator | ã‚·ãƒ³ãƒ—ãƒ«ã€ãƒ‡ãƒãƒƒã‚°å®¹æ˜“ |
| >= 10K ã‚µãƒ³ãƒ—ãƒ« | StreamingAggregator | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã€40K+å¯¾å¿œ |

---

## ä¸¦åˆ—å‡¦ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### loky.ProcessPoolExecutorçµ±åˆ

**æ±ºå®šäº‹é …**ï¼ˆQ24ï¼‰: loky.ProcessPoolExecutorï¼ˆlmpipeã¨çµ±ä¸€ï¼‰

```mermaid
graph TB
    subgraph "MetricCalculator.run()"
        INIT[loky.ProcessPoolExecutor max_workers=4]
        SUBMIT[submit _process_sample]
        FUTURES[Future~MetricResult~ list]
        RESULTS[future.result]
    end
    
    subgraph "Worker Pool"
        W1[Worker 1]
        W2[Worker 2]
        W3[Worker 3]
        W4[Worker 4]
    end
    
    subgraph "å‡¦ç†ãƒ•ãƒ­ãƒ¼"
        TASK[_process_sample dataset, idx]
        LOAD[dataset.get_item idx]
        FILTER[_filter item, category]
        CALC[metric.calculate data]
        RETURN[MetricResult]
    end
    
    subgraph "é›†ç´„"
        AGG_INIT[aggregator.init]
        AGG_UPDATE[aggregator.update state, result]
        AGG_FINAL[aggregator.finalize state]
    end
    
    INIT --> SUBMIT
    SUBMIT --> W1
    SUBMIT --> W2
    SUBMIT --> W3
    SUBMIT --> W4
    
    W1 --> TASK
    W2 --> TASK
    W3 --> TASK
    W4 --> TASK
    
    TASK --> LOAD
    LOAD --> FILTER
    FILTER --> CALC
    CALC --> RETURN
    
    RETURN --> FUTURES
    FUTURES --> RESULTS
    
    RESULTS --> AGG_INIT
    AGG_INIT --> AGG_UPDATE
    AGG_UPDATE --> AGG_FINAL
    
    style INIT fill:#fce4ec
    style AGG_FINAL fill:#e8f5e9
```

**å®Ÿè£…**:

```python
from loky import ProcessPoolExecutor
from concurrent.futures import as_completed

class MetricCalculator:
    def run(self, dataset: SLDataset, workers: int = 4) -> dict:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        
        Args:
            dataset: SLDataset
            workers: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4ï¼‰
        
        Returns:
            é›†ç´„çµæœã®è¾æ›¸
        """
        # AggregatoråˆæœŸåŒ–
        state = self.aggregator.init()
        
        # ä¸¦åˆ—å‡¦ç†
        with ProcessPoolExecutor(max_workers=workers) as executor:
            futures = [
                executor.submit(self._process_sample, dataset, idx)
                for idx in range(len(dataset))
            ]
            
            # é€²æ—è¡¨ç¤ºä»˜ãã§çµæœåé›†
            for future in as_completed(futures):
                try:
                    result = future.result()
                    state = self.aggregator.update(state, result)
                except Exception as e:
                    logger.warning(f"Sample processing failed: {e}")
                    # ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ
        
        # æœ€çµ‚é›†ç´„
        return self.aggregator.finalize(state)
```

---

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
src/cslrtools2/sldataset/metrics/
â”œâ”€â”€ __init__.py ..................... MetricCalculator, MetricLoader export
â”œâ”€â”€ base.py ......................... Metric, LandmarkMetric, MetricResult
â”œâ”€â”€ aggregators.py .................. Aggregator, DefaultAggregator, StreamingAggregator
â”œâ”€â”€ calculator.py ................... MetricCalculatorï¼ˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼‰
â”œâ”€â”€ loader.py ....................... MetricLoaderï¼ˆãƒ—ãƒ©ã‚°ã‚¤ãƒ³ç™ºè¦‹ï¼‰
â”œâ”€â”€ utils.py ........................ ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
â”‚
â””â”€â”€ methods/ ........................ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ï¼‰
    â”œâ”€â”€ __init__.py ................. å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®export
    â”‚
    â”œâ”€â”€ completeness/ ............... Phase 1: å®Œå…¨æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ nan_rate.py ............. NaNRateMetric
    â”‚
    â”œâ”€â”€ temporal/ ................... Phase 2: æ™‚é–“çš„ä¸€è²«æ€§
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ consistency.py .......... TemporalConsistencyMetric
    â”‚   â””â”€â”€ smoothness.py ........... SmoothnessMetric
    â”‚
    â””â”€â”€ anatomical/ ................. Phase 3: éª¨æ ¼åˆ¶ç´„
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ constraints.py .......... AnatomicalConstraintMetric
        â””â”€â”€ bone_length.py .......... BoneLengthVariationMetric
```

**Entry Pointsç™»éŒ²** (`pyproject.toml`):

```toml
[project.entry-points."cslrtools2.metrics"]
"completeness.nan_rate" = "cslrtools2.sldataset.metrics.methods.completeness:NaNRateMetric"
"temporal.consistency" = "cslrtools2.sldataset.metrics.methods.temporal:TemporalConsistencyMetric"
"anatomical.constraints" = "cslrtools2.sldataset.metrics.methods.anatomical:AnatomicalConstraintMetric"
```

---

## å®Ÿè£…ä¾‹

### 1. NaNRateMetricï¼ˆPhase 1ï¼‰

```python
# methods/completeness/nan_rate.py
from __future__ import annotations

import numpy as np
from numpy.typing import NDArray

from ...base import LandmarkMetric, MetricResult


class NaNRateMetric(LandmarkMetric):
    """NaNç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆå®Œå…¨æ€§è©•ä¾¡ï¼‰
    
    ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«ã®NaNå­˜åœ¨ç‡ã‚’è¨ˆç®—:
        nan_rate = mean(any(isnan(frame)))
    
    æ¨å¥¨é–¾å€¤: < 0.2 (20%)
    ã‚¨ãƒ³ã‚¸ãƒ³éä¾å­˜: âœ…
    Ground Truthä¸è¦: âœ…
    å®Ÿè£…æ¨å¥¨åº¦: â­â­â­â­â­
    """
    
    def calculate(self, data: NDArray[np.float32], **kwargs) -> MetricResult:
        """NaNç‡ã‚’è¨ˆç®—
        
        Args:
            data: shape (T, K, D) ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯é…åˆ—
        
        Returns:
            nan_rate ã‚’å«ã‚€ MetricResult
        """
        if not self.validate_inputs(data):
            raise ValueError(f"Invalid input shape: {data.shape}")
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«NaNå­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯
        frames_with_nan = np.any(np.isnan(data), axis=(1, 2))  # shape: (T,)
        nan_rate = np.mean(frames_with_nan)
        
        return MetricResult(
            metric_name='completeness.nan_rate',
            values={
                'nan_rate': float(nan_rate),
                'frames_with_nan': int(np.sum(frames_with_nan))
            },
            metadata={
                'total_frames': data.shape[0],
                'shape': data.shape,
                'threshold_recommended': 0.2
            },
            sample_id=kwargs.get('sample_id', -1)
        )
    
    def get_description(self) -> str:
        return (
            "Calculates the proportion of frames containing at least one NaN value. "
            "Lower is better (0.0 = perfect completeness)."
        )
```

### 2. TemporalConsistencyMetricï¼ˆPhase 2ï¼‰

```python
# methods/temporal/consistency.py
from __future__ import annotations

import numpy as np
from numpy.typing import NDArray

from ...base import LandmarkMetric, MetricResult


class TemporalConsistencyMetric(LandmarkMetric):
    """æ™‚é–“çš„ä¸€è²«æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæ»‘ã‚‰ã‹ã•è©•ä¾¡ï¼‰
    
    åŠ é€Ÿåº¦ã®æ¨™æº–åå·®ã§å‹•ãã®æ»‘ã‚‰ã‹ã•ã‚’è©•ä¾¡:
        velocity = landmarks[1:] - landmarks[:-1]
        acceleration = velocity[1:] - velocity[:-1]
        smoothness = std(acceleration)
    
    ä½ã„ã»ã©æ»‘ã‚‰ã‹: ã‚¸ãƒƒã‚¿ãŒå°‘ãªã„
    ã‚¨ãƒ³ã‚¸ãƒ³éä¾å­˜: âœ…
    å®Ÿè£…æ¨å¥¨åº¦: â­â­â­â­â­
    """
    
    def __init__(self, window_size: int = 5):
        """
        Args:
            window_size: å¹³æ»‘åŒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆæœªä½¿ç”¨ã€å°†æ¥æ‹¡å¼µç”¨ï¼‰
        """
        self.window_size = window_size
    
    def calculate(self, data: NDArray[np.float32], **kwargs) -> MetricResult:
        """æ™‚é–“çš„ä¸€è²«æ€§ã‚’è¨ˆç®—
        
        Args:
            data: shape (T, K, D)
        
        Returns:
            smoothness ã‚’å«ã‚€ MetricResult
        """
        if not self.validate_inputs(data):
            raise ValueError(f"Invalid input shape: {data.shape}")
        
        if data.shape[0] < 3:
            raise ValueError("Need at least 3 frames for acceleration calculation")
        
        # NaNã‚’å«ã‚€ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒã‚¹ã‚¯
        valid_mask = ~np.any(np.isnan(data), axis=(1, 2))
        
        # é€Ÿåº¦è¨ˆç®—
        velocity = np.diff(data, axis=0)  # shape: (T-1, K, D)
        
        # åŠ é€Ÿåº¦è¨ˆç®—
        acceleration = np.diff(velocity, axis=0)  # shape: (T-2, K, D)
        
        # æ»‘ã‚‰ã‹ã•: åŠ é€Ÿåº¦ã®æ¨™æº–åå·®
        smoothness = float(np.nanstd(acceleration))
        
        return MetricResult(
            metric_name='temporal.consistency',
            values={
                'smoothness': smoothness,
                'mean_acceleration': float(np.nanmean(np.abs(acceleration)))
            },
            metadata={
                'total_frames': data.shape[0],
                'valid_frames': int(np.sum(valid_mask)),
                'window_size': self.window_size
            },
            sample_id=kwargs.get('sample_id', -1)
        )
    
    def get_description(self) -> str:
        return (
            "Evaluates temporal consistency by calculating acceleration standard deviation. "
            "Lower values indicate smoother motion (less jitter)."
        )
```

### 3. MetricCalculatorä½¿ç”¨ä¾‹

```python
# ä½¿ç”¨ä¾‹
from cslrtools2.sldataset import SLDataset
from cslrtools2.sldataset.metrics import MetricCalculator
from cslrtools2.sldataset.metrics.aggregators import StreamingAggregator

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰
dataset = SLDataset.from_zarr("path/to/dataset.zarr")

# è¨­å®šï¼ˆYAMLç›¸å½“ï¼‰
from cslrtools2.sldataset.metrics.calculator import CalculationSpec

specs = [
    CalculationSpec(
        method="completeness.nan_rate",
        categories=["pose"]
    ),
    CalculationSpec(
        method="temporal.consistency",
        categories=[
            {"name": "hands", "landmarks": ["left_hand", "right_hand"]}
        ],
        window_size=5
    )
]

# CalculatoråˆæœŸåŒ–
calculator = MetricCalculator(
    specs=specs,
    aggregator=StreamingAggregator()
)

# å®Ÿè¡Œï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
results = calculator.run(dataset, workers=4)

print(results)
# {
#   'pose': {
#     'completeness.nan_rate': {
#       'mean': 0.05,
#       'std': 0.02,
#       'min': 0.0,
#       'max': 0.15,
#       'samples': 40214
#     }
#   },
#   'hands': {
#     'temporal.consistency': {
#       'mean': 0.03,
#       ...
#     }
#   }
# }
```

---

## ã¾ã¨ã‚

### è¨­è¨ˆã®ç‰¹å¾´

| ç‰¹å¾´ | èª¬æ˜ | å®Ÿç¾æ–¹æ³• |
|------|------|----------|
| **è²¬ä»»åˆ†é›¢** | ãƒ‡ãƒ¼ã‚¿ãƒ»è¨ˆç®—ãƒ»é›†ç´„ã‚’åˆ†é›¢ | Metric, Aggregator, Calculatorã®3å±¤ |
| **å‹å®‰å…¨** | é™çš„å‹ãƒã‚§ãƒƒã‚¯å®Œå‚™ | TypedDict, PEP 695 generics |
| **ã‚¨ãƒ³ã‚¸ãƒ³éä¾å­˜** | MediaPipe/OpenPoseç­‰ã«ä¾å­˜ã—ãªã„ | NumPyé…åˆ—ãƒ™ãƒ¼ã‚¹ |
| **ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯èƒ½** | å¤–éƒ¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ å®¹æ˜“ | Entry Points |
| **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡** | 40K+ã‚µãƒ³ãƒ—ãƒ«å¯¾å¿œ | StreamingAggregator |
| **ä¸¦åˆ—å‡¦ç†** | CPUä¸¦åˆ—åŒ– | loky.ProcessPoolExecutor |
| **ã‚¨ãƒ©ãƒ¼è€æ€§** | ä¸€éƒ¨å¤±æ•—ã§ã‚‚ç¶šè¡Œ | try/except + warning log |

### å®Ÿè£…å„ªå…ˆåº¦

1. **Phase 1ï¼ˆMVPï¼‰**: Metric, Aggregator, Calculatoréª¨æ ¼ + NaNRateMetric
2. **Phase 2ï¼ˆä¸¦åˆ—åŒ–ï¼‰**: lokyçµ±åˆ, ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º, ã‚¨ãƒ©ãƒ¼å‡¦ç†
3. **Phase 3ï¼ˆé«˜åº¦åŒ–ï¼‰**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é›†ç´„, ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ, Entry Points
4. **Phase 4ï¼ˆæœ€é©åŒ–ï¼‰**: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯, ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ, è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹

---

**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
- å®Ÿè£…è‰æ¡ˆ: `METRICS_IMPLEMENTATION_DRAFT.md`
- ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è¨­è¨ˆ: `METRICS_CALCULATOR_DESIGN.md`
- å®Œå…¨ç‰ˆQ&A: `metrics_calc_memo.md`
- ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—: `metrics_prototype2/`
